# AI Plan Builder (Tranche 9) â€” LLM provider configuration
# Default behavior: deterministic; no LLM configuration required.

# Enable LLM mode (server-side only)
AI_PLAN_BUILDER_AI_MODE=deterministic

# LLM provider selection
AI_PLAN_BUILDER_LLM_PROVIDER=openai

# Single model id
AI_PLAN_BUILDER_LLM_MODEL=

# OpenAI secret (DO NOT COMMIT REAL VALUES)
OPENAI_API_KEY=

# Optional guardrails
AI_PLAN_BUILDER_LLM_TIMEOUT_MS=20000
AI_PLAN_BUILDER_LLM_MAX_OUTPUT_TOKENS=1200

# Tranche 10: controlled rollout (per-capability overrides)
# Values: inherit | deterministic | llm
AI_PLAN_BUILDER_AI_CAP_SUMMARIZE_INTAKE=inherit
AI_PLAN_BUILDER_AI_CAP_SUGGEST_DRAFT_PLAN=inherit
AI_PLAN_BUILDER_AI_CAP_SUGGEST_PROPOSAL_DIFFS=inherit

# Tranche 10: per-capability token limits (optional)
AI_PLAN_BUILDER_LLM_MAX_OUTPUT_TOKENS_SUMMARIZE_INTAKE=
AI_PLAN_BUILDER_LLM_MAX_OUTPUT_TOKENS_SUGGEST_DRAFT_PLAN=
AI_PLAN_BUILDER_LLM_MAX_OUTPUT_TOKENS_SUGGEST_PROPOSAL_DIFFS=

# Tranche 10: retry/rate limit policy (server-side)
# Retry count = number of retries (not attempts). Default 1; max 2.
AI_PLAN_BUILDER_LLM_RETRY_COUNT=1

# Budget: max LLM invocations per actor per hour.
AI_PLAN_BUILDER_LLM_RATE_LIMIT_PER_HOUR=20
